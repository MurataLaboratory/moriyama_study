{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seq_with_torchtext.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMAoEYMmZIR7Swx8xgW6hQV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"K1fFM0v47L6x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606369187517,"user_tz":-540,"elapsed":680,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"989001ed-48c0-4b05-84a6-712a646c92d4"},"source":["print('hello world')"],"execution_count":40,"outputs":[{"output_type":"stream","text":["hello world\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JYxc4mB29niT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606369188128,"user_tz":-540,"elapsed":1280,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"b67ff6d9-6893-4105-8144-cc75de964a72"},"source":["# Google Drivceのマウント\n","from google.colab import drive\n","drive.mount('/content/dirve')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/dirve; to attempt to forcibly remount, call drive.mount(\"/content/dirve\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ptAoqK139GWM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606369190251,"user_tz":-540,"elapsed":3396,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"5331bf36-0c83-4f39-c6a2-62ba139f9940"},"source":["!pip install torchtext==0.6.0"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.6/dist-packages (0.6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.18.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (0.1.94)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.11.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"grk5TlOS7Rh4","executionInfo":{"status":"ok","timestamp":1606369190252,"user_tz":-540,"elapsed":3390,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["# 必要なモジュールのインポート\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torchtext.data import Field, BucketIterator\n","\n","import numpy as np\n","\n","import random\n","import math\n","import time\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"hV9m14jC8_As","executionInfo":{"status":"ok","timestamp":1606369190253,"user_tz":-540,"elapsed":3386,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"RO1VkLhS9Aur","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606369192473,"user_tz":-540,"elapsed":5601,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"6758e965-f679-4150-da1b-ed43571f3437"},"source":["!pip install janome"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: janome in /usr/local/lib/python3.6/dist-packages (0.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LSY1Tjxk9Q5l","executionInfo":{"status":"ok","timestamp":1606369192474,"user_tz":-540,"elapsed":5595,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["import janome\n","from janome.tokenizer import Tokenizer\n","j_t = Tokenizer()\n","\n","# 日本語を単語に分割したリストを返す関数\n","def tokenizer(text):\n","  return [tok for tok in j_t.tokenize(text, wakati=True)]"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlJ-5vHV9gM0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606369192475,"user_tz":-540,"elapsed":5590,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"f3741a80-5a05-4204-bc82-96af12550c93"},"source":["tokenizer(\"今日は曇りです\")"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['今日', 'は', '曇り', 'です']"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"P7D0kRBz-Xcp","executionInfo":{"status":"ok","timestamp":1606369192476,"user_tz":-540,"elapsed":5583,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["import torchtext\n","import torch\n","from torchtext import data\n","from torchtext import datasets"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"D0U1LAH39jdW","executionInfo":{"status":"ok","timestamp":1606369192476,"user_tz":-540,"elapsed":5579,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["# pytorchのデータフィードの定義（重要！！）\n","SRC = data.Field(sequential=True, tokenize = tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\n","TRG = data.Field(sequential=True, tokenize = tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"ALwCMyIr4bnd","executionInfo":{"status":"ok","timestamp":1606369192477,"user_tz":-540,"elapsed":5576,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["# 重複のないデータセットか重複のあるデータセットを選ぶ\n","# flagがTrueの時重複のないデータを返す\n","def choose_dataset(flag = False):\n","  if flag:\n","    train, val, test = data.TabularDataset.splits(\n","        path=\"/content/dirve/My Drive/Colab Notebooks/data/\", train='one_train.tsv',\n","        validation='one_val.tsv', test='one_test.tsv', format='tsv',\n","        fields=[('SRC', SRC), ('TRG', TRG)])\n","    filename = \"/content/dirve/My Drive/Colab Notebooks/csv/one_result_Seq2seq.csv\"\n","  else:\n","    train, val, test = data.TabularDataset.splits(\n","        path=\"/content/dirve/My Drive/Colab Notebooks/data/\", train='train.tsv',\n","        validation='val.tsv', test='test.tsv', format='tsv',\n","        fields=[('SRC', SRC), ('TRG', TRG)])\n","    filename = \"/content/dirve/My Drive/Colab Notebooks/csv/result_Seq2seq.csv\"\n","  \n","  return train, val, test, filename"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRnr3e9B-VXJ","executionInfo":{"status":"ok","timestamp":1606369227286,"user_tz":-540,"elapsed":40381,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["train, val, test, filename = choose_dataset(False)"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"AhRWZjOZ-ffs","executionInfo":{"status":"ok","timestamp":1606369227289,"user_tz":-540,"elapsed":40380,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["# 辞書の作成\n","SRC.build_vocab(train)\n","TRG.build_vocab(train)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYeVn3Sr-9b-","executionInfo":{"status":"ok","timestamp":1606369227289,"user_tz":-540,"elapsed":40376,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["# 各データをバッチ化する\n","# データの総数を割れる数にしないと学習時にエラーを吐く\n","train_batch_size = 50\n","test_batch_size = 32\n","eval_batch_size = 2\n","train_iter, val_iter, test_iter = data.BucketIterator.splits((train, val, test), sort = False,  batch_sizes = (train_batch_size,eval_batch_size, test_batch_size), device= device)"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuqR4n4E_EIi","executionInfo":{"status":"ok","timestamp":1606369227290,"user_tz":-540,"elapsed":40373,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["class Encoder(nn.Module):\n","  # Encoder層の設定\n","  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","    super().__init__()\n","    # 埋め込み層の次元数\n","    self.hid_dim = hid_dim\n","    # LSTMレイヤの数\n","    self.n_layers = n_layers\n","    # 埋め込み層の作成\n","    self.embedding = nn.Embedding(input_dim, emb_dim)\n","    # LSTM層の作成\n","    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, src):\n","    # 単語を分散表現に変換する\n","    embedded = self.dropout(self.embedding(src))\n","    # LSTMに単語を入力して、Encoderからの出力とする\n","    outputs, (hidden, cell) = self.rnn(embedded)\n","    return hidden, cell"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYMjBKWIAJa0","executionInfo":{"status":"ok","timestamp":1606369227291,"user_tz":-540,"elapsed":40371,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["class Decoder(nn.Module):\n","  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","    super().__init__()\n","    # 出力の次元数（単語数と同じ）\n","    self.output_dim = output_dim\n","    # 隠れ層の数\n","    self.hid_dim = hid_dim\n","    # レイヤ数\n","    self.n_layers = n_layers\n","    # 埋め込み層\n","    self.embedding = nn.Embedding(output_dim, emb_dim)\n","    # LSTM\n","    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n","    # 出力をするためのやつ\n","    self.fc_out = nn.Linear(hid_dim, output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","  \n","  def forward(self, input, hidden, cell):\n","    # 入力を整形する\n","    input = input.unsqueeze(0)\n","    # 分散表現に変換\n","    embedded = self.dropout(self.embedding(input))\n","    # Decoderからの出力を得る\n","    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","    # 正規化する\n","    prediction = self.fc_out(output.squeeze(0))\n","\n","    return prediction, hidden, cell"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKegU_nlDNTr","executionInfo":{"status":"ok","timestamp":1606369227291,"user_tz":-540,"elapsed":40366,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["class Seq2Seq(nn.Module):\n","  def __init__(self, encoder, decoder, device):\n","    super().__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.device = device\n","\n","  def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","    batch_size = trg.shape[1]\n","    trg_len = trg.shape[0]\n","    trg_vocab_size = self.decoder.output_dim\n","\n","    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","    hidden, cell = self.encoder(src)\n","\n","    output = trg[0,:]\n","\n","    for t in range(1, trg_len):\n","      output, hidden, cell = self.decoder(output, hidden, cell)\n","\n","      outputs[t] = output\n","      teacher_force = random.random() < teacher_forcing_ratio\n","      top1 = output.argmax(1)\n","      output = (trg[t] if teacher_force else top1)\n","\n","    return outputs"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"wiQtw_R4Endo","executionInfo":{"status":"ok","timestamp":1606369227557,"user_tz":-540,"elapsed":40629,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 768\n","DEC_EMB_DIM = 256\n","ENC_HID_DIM = 256\n","DEC_HID_DIM = 256\n","N_LAYERS = 2\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n","\n","model = Seq2Seq(enc, dec, device).to(device)"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0-nOvWpEzoW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606369227558,"user_tz":-540,"elapsed":40624,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"b24162cc-c9be-4736-fbbc-788a141deb7c"},"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","\n","model.apply(init_weights)"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(3652, 768)\n","    (rnn): LSTM(768, 256, num_layers=2, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(2349, 256)\n","    (rnn): LSTM(256, 256, num_layers=2, dropout=0.5)\n","    (fc_out): Linear(in_features=256, out_features=2349, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"QiIkh4XVFP4y","executionInfo":{"status":"ok","timestamp":1606369227559,"user_tz":-540,"elapsed":40619,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["optimizer = optim.Adam(model.parameters())"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLbky7ZvFTqh","executionInfo":{"status":"ok","timestamp":1606369227559,"user_tz":-540,"elapsed":40614,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = SRC_PAD_IDX)"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtfRQ4RsFWaQ","executionInfo":{"status":"ok","timestamp":1606369227560,"user_tz":-540,"elapsed":40610,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["def train(model, iterator, optimizer, criterion, clip):\n","  model.train()\n","\n","  epoch_loss = 0\n","\n","  for i, batch in enumerate(iterator):\n","\n","    src = batch.SRC\n","    trg = batch.TRG\n","    optimizer.zero_grad()\n","\n","    output = model(src, trg)\n","\n","    #print(\"output size:\", output.size())\n","    #print(\"target size:\", trg.size())\n","    output_dim = output.shape[-1]\n","    output = output[:].view(-1, output_dim)\n","    trg = trg[:].view(-1)\n","    loss = criterion(output, trg)\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","\n","  return epoch_loss / len(iterator)"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"VC3CzZF8GVub","executionInfo":{"status":"ok","timestamp":1606369227560,"user_tz":-540,"elapsed":40605,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["def evaluate(model, iterator, criterion):\n","  model.eval()\n","\n","  epoch_loss = 0\n","\n","  with torch.no_grad():\n","\n","    for i, batch in enumerate(iterator):\n","\n","      src = batch.SRC\n","      trg = batch.TRG\n","\n","      output = model(src, trg)\n","\n","      output_dim = output.shape[-1]\n","\n","      output = output[:].view(-1, output_dim)\n","      trg = trg[:].view(-1)\n","\n","      loss = criterion(output, trg)\n","      epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJ0ljjrQJTBw","executionInfo":{"status":"ok","timestamp":1606369227561,"user_tz":-540,"elapsed":40602,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins*60))\n","  return elapsed_mins, elapsed_secs"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZtBtHvgJx7D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"595719f6-f67f-43c0-bfca-9509d5d533c9"},"source":["epochs = 300\n","clip = 1\n","best_model = None\n","\n","\n","best_valid_loss = float('inf')\n","best_model = None\n","for epoch in range(epochs):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iter, optimizer, criterion, clip)\n","    valid_loss = evaluate(model, val_iter, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        best_model = model\n","        #torch.save(model.state_dict(), 'tut1-model.pt')\n","    \n","    print(\"-\"*65)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"],"name":"stderr"},{"output_type":"stream","text":["-----------------------------------------------------------------\n","Epoch: 01 | Time: 0m 14s | Train Loss: 4.427 | Val. Loss: 4.205\n","-----------------------------------------------------------------\n","Epoch: 02 | Time: 0m 14s | Train Loss: 4.108 | Val. Loss: 4.283\n","-----------------------------------------------------------------\n","Epoch: 03 | Time: 0m 14s | Train Loss: 4.040 | Val. Loss: 4.089\n","-----------------------------------------------------------------\n","Epoch: 04 | Time: 0m 14s | Train Loss: 3.988 | Val. Loss: 4.084\n","-----------------------------------------------------------------\n","Epoch: 05 | Time: 0m 14s | Train Loss: 3.951 | Val. Loss: 4.085\n","-----------------------------------------------------------------\n","Epoch: 06 | Time: 0m 14s | Train Loss: 3.911 | Val. Loss: 4.108\n","-----------------------------------------------------------------\n","Epoch: 07 | Time: 0m 14s | Train Loss: 3.921 | Val. Loss: 4.072\n","-----------------------------------------------------------------\n","Epoch: 08 | Time: 0m 14s | Train Loss: 3.890 | Val. Loss: 4.097\n","-----------------------------------------------------------------\n","Epoch: 09 | Time: 0m 14s | Train Loss: 3.879 | Val. Loss: 4.066\n","-----------------------------------------------------------------\n","Epoch: 10 | Time: 0m 14s | Train Loss: 3.848 | Val. Loss: 4.063\n","-----------------------------------------------------------------\n","Epoch: 11 | Time: 0m 14s | Train Loss: 3.841 | Val. Loss: 4.057\n","-----------------------------------------------------------------\n","Epoch: 12 | Time: 0m 14s | Train Loss: 3.828 | Val. Loss: 4.024\n","-----------------------------------------------------------------\n","Epoch: 13 | Time: 0m 14s | Train Loss: 3.804 | Val. Loss: 4.019\n","-----------------------------------------------------------------\n","Epoch: 14 | Time: 0m 14s | Train Loss: 3.786 | Val. Loss: 4.038\n","-----------------------------------------------------------------\n","Epoch: 15 | Time: 0m 14s | Train Loss: 3.761 | Val. Loss: 4.053\n","-----------------------------------------------------------------\n","Epoch: 16 | Time: 0m 14s | Train Loss: 3.732 | Val. Loss: 4.039\n","-----------------------------------------------------------------\n","Epoch: 17 | Time: 0m 14s | Train Loss: 3.710 | Val. Loss: 4.029\n","-----------------------------------------------------------------\n","Epoch: 18 | Time: 0m 14s | Train Loss: 3.697 | Val. Loss: 4.030\n","-----------------------------------------------------------------\n","Epoch: 19 | Time: 0m 14s | Train Loss: 3.668 | Val. Loss: 4.061\n","-----------------------------------------------------------------\n","Epoch: 20 | Time: 0m 14s | Train Loss: 3.654 | Val. Loss: 4.061\n","-----------------------------------------------------------------\n","Epoch: 21 | Time: 0m 14s | Train Loss: 3.616 | Val. Loss: 4.044\n","-----------------------------------------------------------------\n","Epoch: 22 | Time: 0m 14s | Train Loss: 3.600 | Val. Loss: 4.066\n","-----------------------------------------------------------------\n","Epoch: 23 | Time: 0m 14s | Train Loss: 3.600 | Val. Loss: 4.054\n","-----------------------------------------------------------------\n","Epoch: 24 | Time: 0m 14s | Train Loss: 3.588 | Val. Loss: 4.065\n","-----------------------------------------------------------------\n","Epoch: 25 | Time: 0m 14s | Train Loss: 3.553 | Val. Loss: 4.057\n","-----------------------------------------------------------------\n","Epoch: 26 | Time: 0m 14s | Train Loss: 3.539 | Val. Loss: 4.049\n","-----------------------------------------------------------------\n","Epoch: 27 | Time: 0m 14s | Train Loss: 3.506 | Val. Loss: 4.060\n","-----------------------------------------------------------------\n","Epoch: 28 | Time: 0m 14s | Train Loss: 3.509 | Val. Loss: 4.095\n","-----------------------------------------------------------------\n","Epoch: 29 | Time: 0m 14s | Train Loss: 3.472 | Val. Loss: 4.057\n","-----------------------------------------------------------------\n","Epoch: 30 | Time: 0m 14s | Train Loss: 3.486 | Val. Loss: 4.105\n","-----------------------------------------------------------------\n","Epoch: 31 | Time: 0m 14s | Train Loss: 3.436 | Val. Loss: 4.092\n","-----------------------------------------------------------------\n","Epoch: 32 | Time: 0m 14s | Train Loss: 3.426 | Val. Loss: 4.111\n","-----------------------------------------------------------------\n","Epoch: 33 | Time: 0m 14s | Train Loss: 3.395 | Val. Loss: 4.103\n","-----------------------------------------------------------------\n","Epoch: 34 | Time: 0m 14s | Train Loss: 3.385 | Val. Loss: 4.099\n","-----------------------------------------------------------------\n","Epoch: 35 | Time: 0m 14s | Train Loss: 3.381 | Val. Loss: 4.124\n","-----------------------------------------------------------------\n","Epoch: 36 | Time: 0m 14s | Train Loss: 3.364 | Val. Loss: 4.172\n","-----------------------------------------------------------------\n","Epoch: 37 | Time: 0m 14s | Train Loss: 3.346 | Val. Loss: 4.165\n","-----------------------------------------------------------------\n","Epoch: 38 | Time: 0m 14s | Train Loss: 3.326 | Val. Loss: 4.177\n","-----------------------------------------------------------------\n","Epoch: 39 | Time: 0m 14s | Train Loss: 3.338 | Val. Loss: 4.167\n","-----------------------------------------------------------------\n","Epoch: 40 | Time: 0m 14s | Train Loss: 3.309 | Val. Loss: 4.184\n","-----------------------------------------------------------------\n","Epoch: 41 | Time: 0m 14s | Train Loss: 3.283 | Val. Loss: 4.212\n","-----------------------------------------------------------------\n","Epoch: 42 | Time: 0m 14s | Train Loss: 3.268 | Val. Loss: 4.208\n","-----------------------------------------------------------------\n","Epoch: 43 | Time: 0m 14s | Train Loss: 3.247 | Val. Loss: 4.196\n","-----------------------------------------------------------------\n","Epoch: 44 | Time: 0m 14s | Train Loss: 3.226 | Val. Loss: 4.242\n","-----------------------------------------------------------------\n","Epoch: 45 | Time: 0m 14s | Train Loss: 3.212 | Val. Loss: 4.234\n","-----------------------------------------------------------------\n","Epoch: 46 | Time: 0m 14s | Train Loss: 3.207 | Val. Loss: 4.261\n","-----------------------------------------------------------------\n","Epoch: 47 | Time: 0m 14s | Train Loss: 3.211 | Val. Loss: 4.246\n","-----------------------------------------------------------------\n","Epoch: 48 | Time: 0m 14s | Train Loss: 3.183 | Val. Loss: 4.280\n","-----------------------------------------------------------------\n","Epoch: 49 | Time: 0m 14s | Train Loss: 3.167 | Val. Loss: 4.303\n","-----------------------------------------------------------------\n","Epoch: 50 | Time: 0m 14s | Train Loss: 3.158 | Val. Loss: 4.310\n","-----------------------------------------------------------------\n","Epoch: 51 | Time: 0m 14s | Train Loss: 3.130 | Val. Loss: 4.295\n","-----------------------------------------------------------------\n","Epoch: 52 | Time: 0m 14s | Train Loss: 3.132 | Val. Loss: 4.294\n","-----------------------------------------------------------------\n","Epoch: 53 | Time: 0m 14s | Train Loss: 3.103 | Val. Loss: 4.293\n","-----------------------------------------------------------------\n","Epoch: 54 | Time: 0m 14s | Train Loss: 3.115 | Val. Loss: 4.342\n","-----------------------------------------------------------------\n","Epoch: 55 | Time: 0m 14s | Train Loss: 3.100 | Val. Loss: 4.348\n","-----------------------------------------------------------------\n","Epoch: 56 | Time: 0m 14s | Train Loss: 3.075 | Val. Loss: 4.356\n","-----------------------------------------------------------------\n","Epoch: 57 | Time: 0m 14s | Train Loss: 3.074 | Val. Loss: 4.339\n","-----------------------------------------------------------------\n","Epoch: 58 | Time: 0m 14s | Train Loss: 3.052 | Val. Loss: 4.381\n","-----------------------------------------------------------------\n","Epoch: 59 | Time: 0m 14s | Train Loss: 3.034 | Val. Loss: 4.387\n","-----------------------------------------------------------------\n","Epoch: 60 | Time: 0m 14s | Train Loss: 3.034 | Val. Loss: 4.428\n","-----------------------------------------------------------------\n","Epoch: 61 | Time: 0m 14s | Train Loss: 3.015 | Val. Loss: 4.426\n","-----------------------------------------------------------------\n","Epoch: 62 | Time: 0m 14s | Train Loss: 3.017 | Val. Loss: 4.369\n","-----------------------------------------------------------------\n","Epoch: 63 | Time: 0m 14s | Train Loss: 3.005 | Val. Loss: 4.429\n","-----------------------------------------------------------------\n","Epoch: 64 | Time: 0m 14s | Train Loss: 2.994 | Val. Loss: 4.420\n","-----------------------------------------------------------------\n","Epoch: 65 | Time: 0m 14s | Train Loss: 2.998 | Val. Loss: 4.461\n","-----------------------------------------------------------------\n","Epoch: 66 | Time: 0m 14s | Train Loss: 2.968 | Val. Loss: 4.452\n","-----------------------------------------------------------------\n","Epoch: 67 | Time: 0m 14s | Train Loss: 2.983 | Val. Loss: 4.465\n","-----------------------------------------------------------------\n","Epoch: 68 | Time: 0m 14s | Train Loss: 2.942 | Val. Loss: 4.481\n","-----------------------------------------------------------------\n","Epoch: 69 | Time: 0m 14s | Train Loss: 2.952 | Val. Loss: 4.482\n","-----------------------------------------------------------------\n","Epoch: 70 | Time: 0m 14s | Train Loss: 2.945 | Val. Loss: 4.515\n","-----------------------------------------------------------------\n","Epoch: 71 | Time: 0m 14s | Train Loss: 2.936 | Val. Loss: 4.487\n","-----------------------------------------------------------------\n","Epoch: 72 | Time: 0m 14s | Train Loss: 2.921 | Val. Loss: 4.512\n","-----------------------------------------------------------------\n","Epoch: 73 | Time: 0m 14s | Train Loss: 2.912 | Val. Loss: 4.516\n","-----------------------------------------------------------------\n","Epoch: 74 | Time: 0m 14s | Train Loss: 2.897 | Val. Loss: 4.544\n","-----------------------------------------------------------------\n","Epoch: 75 | Time: 0m 14s | Train Loss: 2.883 | Val. Loss: 4.563\n","-----------------------------------------------------------------\n","Epoch: 76 | Time: 0m 14s | Train Loss: 2.877 | Val. Loss: 4.542\n","-----------------------------------------------------------------\n","Epoch: 77 | Time: 0m 14s | Train Loss: 2.883 | Val. Loss: 4.571\n","-----------------------------------------------------------------\n","Epoch: 78 | Time: 0m 14s | Train Loss: 2.872 | Val. Loss: 4.606\n","-----------------------------------------------------------------\n","Epoch: 79 | Time: 0m 14s | Train Loss: 2.867 | Val. Loss: 4.596\n","-----------------------------------------------------------------\n","Epoch: 80 | Time: 0m 14s | Train Loss: 2.865 | Val. Loss: 4.643\n","-----------------------------------------------------------------\n","Epoch: 81 | Time: 0m 14s | Train Loss: 2.860 | Val. Loss: 4.608\n","-----------------------------------------------------------------\n","Epoch: 82 | Time: 0m 14s | Train Loss: 2.838 | Val. Loss: 4.613\n","-----------------------------------------------------------------\n","Epoch: 83 | Time: 0m 14s | Train Loss: 2.819 | Val. Loss: 4.655\n","-----------------------------------------------------------------\n","Epoch: 84 | Time: 0m 14s | Train Loss: 2.817 | Val. Loss: 4.646\n","-----------------------------------------------------------------\n","Epoch: 85 | Time: 0m 14s | Train Loss: 2.847 | Val. Loss: 4.651\n","-----------------------------------------------------------------\n","Epoch: 86 | Time: 0m 14s | Train Loss: 2.782 | Val. Loss: 4.629\n","-----------------------------------------------------------------\n","Epoch: 87 | Time: 0m 14s | Train Loss: 2.767 | Val. Loss: 4.684\n","-----------------------------------------------------------------\n","Epoch: 88 | Time: 0m 14s | Train Loss: 2.816 | Val. Loss: 4.692\n","-----------------------------------------------------------------\n","Epoch: 89 | Time: 0m 14s | Train Loss: 2.786 | Val. Loss: 4.638\n","-----------------------------------------------------------------\n","Epoch: 90 | Time: 0m 14s | Train Loss: 2.811 | Val. Loss: 4.718\n","-----------------------------------------------------------------\n","Epoch: 91 | Time: 0m 14s | Train Loss: 2.785 | Val. Loss: 4.715\n","-----------------------------------------------------------------\n","Epoch: 92 | Time: 0m 14s | Train Loss: 2.800 | Val. Loss: 4.719\n","-----------------------------------------------------------------\n","Epoch: 93 | Time: 0m 14s | Train Loss: 2.777 | Val. Loss: 4.733\n","-----------------------------------------------------------------\n","Epoch: 94 | Time: 0m 14s | Train Loss: 2.751 | Val. Loss: 4.739\n","-----------------------------------------------------------------\n","Epoch: 95 | Time: 0m 14s | Train Loss: 2.741 | Val. Loss: 4.774\n","-----------------------------------------------------------------\n","Epoch: 96 | Time: 0m 14s | Train Loss: 2.739 | Val. Loss: 4.779\n","-----------------------------------------------------------------\n","Epoch: 97 | Time: 0m 14s | Train Loss: 2.751 | Val. Loss: 4.812\n","-----------------------------------------------------------------\n","Epoch: 98 | Time: 0m 14s | Train Loss: 2.747 | Val. Loss: 4.773\n","-----------------------------------------------------------------\n","Epoch: 99 | Time: 0m 14s | Train Loss: 2.734 | Val. Loss: 4.774\n","-----------------------------------------------------------------\n","Epoch: 100 | Time: 0m 14s | Train Loss: 2.713 | Val. Loss: 4.799\n","-----------------------------------------------------------------\n","Epoch: 101 | Time: 0m 14s | Train Loss: 2.718 | Val. Loss: 4.801\n","-----------------------------------------------------------------\n","Epoch: 102 | Time: 0m 14s | Train Loss: 2.688 | Val. Loss: 4.771\n","-----------------------------------------------------------------\n","Epoch: 103 | Time: 0m 14s | Train Loss: 2.723 | Val. Loss: 4.814\n","-----------------------------------------------------------------\n","Epoch: 104 | Time: 0m 14s | Train Loss: 2.713 | Val. Loss: 4.838\n","-----------------------------------------------------------------\n","Epoch: 105 | Time: 0m 14s | Train Loss: 2.697 | Val. Loss: 4.838\n","-----------------------------------------------------------------\n","Epoch: 106 | Time: 0m 14s | Train Loss: 2.685 | Val. Loss: 4.816\n","-----------------------------------------------------------------\n","Epoch: 107 | Time: 0m 15s | Train Loss: 2.687 | Val. Loss: 4.870\n","-----------------------------------------------------------------\n","Epoch: 108 | Time: 0m 15s | Train Loss: 2.692 | Val. Loss: 4.858\n","-----------------------------------------------------------------\n","Epoch: 109 | Time: 0m 15s | Train Loss: 2.681 | Val. Loss: 4.886\n","-----------------------------------------------------------------\n","Epoch: 110 | Time: 0m 15s | Train Loss: 2.659 | Val. Loss: 4.896\n","-----------------------------------------------------------------\n","Epoch: 111 | Time: 0m 15s | Train Loss: 2.671 | Val. Loss: 4.889\n","-----------------------------------------------------------------\n","Epoch: 112 | Time: 0m 15s | Train Loss: 2.679 | Val. Loss: 4.873\n","-----------------------------------------------------------------\n","Epoch: 113 | Time: 0m 15s | Train Loss: 2.684 | Val. Loss: 4.921\n","-----------------------------------------------------------------\n","Epoch: 114 | Time: 0m 14s | Train Loss: 2.648 | Val. Loss: 4.930\n","-----------------------------------------------------------------\n","Epoch: 115 | Time: 0m 14s | Train Loss: 2.641 | Val. Loss: 4.891\n","-----------------------------------------------------------------\n","Epoch: 116 | Time: 0m 15s | Train Loss: 2.641 | Val. Loss: 4.902\n","-----------------------------------------------------------------\n","Epoch: 117 | Time: 0m 15s | Train Loss: 2.646 | Val. Loss: 4.886\n","-----------------------------------------------------------------\n","Epoch: 118 | Time: 0m 15s | Train Loss: 2.650 | Val. Loss: 4.944\n","-----------------------------------------------------------------\n","Epoch: 119 | Time: 0m 15s | Train Loss: 2.616 | Val. Loss: 4.946\n","-----------------------------------------------------------------\n","Epoch: 120 | Time: 0m 15s | Train Loss: 2.626 | Val. Loss: 4.917\n","-----------------------------------------------------------------\n","Epoch: 121 | Time: 0m 15s | Train Loss: 2.621 | Val. Loss: 4.938\n","-----------------------------------------------------------------\n","Epoch: 122 | Time: 0m 14s | Train Loss: 2.629 | Val. Loss: 4.940\n","-----------------------------------------------------------------\n","Epoch: 123 | Time: 0m 14s | Train Loss: 2.623 | Val. Loss: 4.986\n","-----------------------------------------------------------------\n","Epoch: 124 | Time: 0m 15s | Train Loss: 2.610 | Val. Loss: 4.989\n","-----------------------------------------------------------------\n","Epoch: 125 | Time: 0m 15s | Train Loss: 2.608 | Val. Loss: 5.032\n","-----------------------------------------------------------------\n","Epoch: 126 | Time: 0m 15s | Train Loss: 2.577 | Val. Loss: 4.981\n","-----------------------------------------------------------------\n","Epoch: 127 | Time: 0m 14s | Train Loss: 2.583 | Val. Loss: 5.025\n","-----------------------------------------------------------------\n","Epoch: 128 | Time: 0m 14s | Train Loss: 2.602 | Val. Loss: 5.033\n","-----------------------------------------------------------------\n","Epoch: 129 | Time: 0m 14s | Train Loss: 2.602 | Val. Loss: 5.027\n","-----------------------------------------------------------------\n","Epoch: 130 | Time: 0m 14s | Train Loss: 2.591 | Val. Loss: 5.007\n","-----------------------------------------------------------------\n","Epoch: 131 | Time: 0m 14s | Train Loss: 2.602 | Val. Loss: 5.046\n","-----------------------------------------------------------------\n","Epoch: 132 | Time: 0m 14s | Train Loss: 2.598 | Val. Loss: 5.056\n","-----------------------------------------------------------------\n","Epoch: 133 | Time: 0m 14s | Train Loss: 2.553 | Val. Loss: 5.023\n","-----------------------------------------------------------------\n","Epoch: 134 | Time: 0m 14s | Train Loss: 2.594 | Val. Loss: 5.050\n","-----------------------------------------------------------------\n","Epoch: 135 | Time: 0m 14s | Train Loss: 2.599 | Val. Loss: 5.050\n","-----------------------------------------------------------------\n","Epoch: 136 | Time: 0m 14s | Train Loss: 2.560 | Val. Loss: 5.020\n","-----------------------------------------------------------------\n","Epoch: 137 | Time: 0m 14s | Train Loss: 2.560 | Val. Loss: 5.071\n","-----------------------------------------------------------------\n","Epoch: 138 | Time: 0m 14s | Train Loss: 2.555 | Val. Loss: 5.068\n","-----------------------------------------------------------------\n","Epoch: 139 | Time: 0m 14s | Train Loss: 2.524 | Val. Loss: 5.110\n","-----------------------------------------------------------------\n","Epoch: 140 | Time: 0m 14s | Train Loss: 2.555 | Val. Loss: 5.079\n","-----------------------------------------------------------------\n","Epoch: 141 | Time: 0m 14s | Train Loss: 2.547 | Val. Loss: 5.118\n","-----------------------------------------------------------------\n","Epoch: 142 | Time: 0m 14s | Train Loss: 2.551 | Val. Loss: 5.111\n","-----------------------------------------------------------------\n","Epoch: 143 | Time: 0m 14s | Train Loss: 2.549 | Val. Loss: 5.097\n","-----------------------------------------------------------------\n","Epoch: 144 | Time: 0m 14s | Train Loss: 2.562 | Val. Loss: 5.125\n","-----------------------------------------------------------------\n","Epoch: 145 | Time: 0m 14s | Train Loss: 2.530 | Val. Loss: 5.106\n","-----------------------------------------------------------------\n","Epoch: 146 | Time: 0m 14s | Train Loss: 2.544 | Val. Loss: 5.133\n","-----------------------------------------------------------------\n","Epoch: 147 | Time: 0m 14s | Train Loss: 2.545 | Val. Loss: 5.130\n","-----------------------------------------------------------------\n","Epoch: 148 | Time: 0m 14s | Train Loss: 2.504 | Val. Loss: 5.157\n","-----------------------------------------------------------------\n","Epoch: 149 | Time: 0m 14s | Train Loss: 2.525 | Val. Loss: 5.155\n","-----------------------------------------------------------------\n","Epoch: 150 | Time: 0m 14s | Train Loss: 2.518 | Val. Loss: 5.163\n","-----------------------------------------------------------------\n","Epoch: 151 | Time: 0m 14s | Train Loss: 2.507 | Val. Loss: 5.155\n","-----------------------------------------------------------------\n","Epoch: 152 | Time: 0m 14s | Train Loss: 2.515 | Val. Loss: 5.154\n","-----------------------------------------------------------------\n","Epoch: 153 | Time: 0m 14s | Train Loss: 2.526 | Val. Loss: 5.127\n","-----------------------------------------------------------------\n","Epoch: 154 | Time: 0m 14s | Train Loss: 2.532 | Val. Loss: 5.177\n","-----------------------------------------------------------------\n","Epoch: 155 | Time: 0m 14s | Train Loss: 2.518 | Val. Loss: 5.230\n","-----------------------------------------------------------------\n","Epoch: 156 | Time: 0m 14s | Train Loss: 2.512 | Val. Loss: 5.174\n","-----------------------------------------------------------------\n","Epoch: 157 | Time: 0m 14s | Train Loss: 2.511 | Val. Loss: 5.233\n","-----------------------------------------------------------------\n","Epoch: 158 | Time: 0m 14s | Train Loss: 2.499 | Val. Loss: 5.222\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kghORfY83ZRj"},"source":["torch.save(model.state_dict(), '/content/dirve/My Drive/Colab Notebooks/model/seq2seq.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckjXccGO3Z1_"},"source":["model.state_dict(torch.load(\"/content/dirve/My Drive/Colab Notebooks/model/seq2seq.pth\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wnxyMwrJ0uK"},"source":["def gen_sentence(sentence, src_field, trg_field, model, max_len = 50):\n","  model.eval()\n","\n","  tokens = [src_field.init_token] + tokenizer(sentence) + [src_field.eos_token]\n","  \n","  src_index = [src_field.vocab.stoi[i] for i in tokens]\n","  src_tensor = torch.LongTensor(src_index).unsqueeze(1).to(device)\n","  src_len = torch.LongTensor([len(src_index)]).to(device)\n","  with torch.no_grad():\n","    hidden, cell = model.encoder(src_tensor)\n","  \n","  trg_index = [trg_field.vocab.stoi[trg_field.init_token]]\n","  for i in range(max_len):\n","    trg_tensor = torch.LongTensor([trg_index[-1]]).to(device)\n","    with torch.no_grad():\n","      output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n","    \n","    pred_token = output.argmax(1).item()\n","    trg_index.append(pred_token)\n","    if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","      break\n","\n","  trg_tokens = [trg_field.vocab.itos[i] for i in trg_index]\n","  if len(trg_tokens) == 2:\n","    print(trg_tokens)\n","    trg_tokens = [\" \"]\n","    trg_tokens = [src_field.init_token] + trg_tokens + [src_field.eos_token]\n","  return trg_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgvU7NIJqDud"},"source":["def gen_sentence_list(path): \n","  col, pred = [], []\n","  input, output = [], []\n","  with open(path, mode = 'r') as f:\n","    for file_list in f:\n","      col.append(file_list.split('\\t'))\n","  for i in col:\n","    input.append(i[0])\n","    output.append(i[1])\n","\n","  for sentence in input:\n","    pred.append(gen_sentence(sentence, SRC, TRG, model))\n","  return input, output, pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4altRgQ3qGJN"},"source":["path = \"/content/dirve/My Drive/Colab Notebooks/data/test.tsv\"\n","test_input, test_output, test_pred = gen_sentence_list(path)\n","path = \"/content/dirve/My Drive/Colab Notebooks/data/train.tsv\"\n","train_input, train_output, train_pred = gen_sentence_list(path)\n","path = \"/content/dirve/My Drive/Colab Notebooks/data/val.tsv\"\n","val_input, val_output, val_pred = gen_sentence_list(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_fFU1apNN8p"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMmGXHSGNU1y"},"source":["def convert_list_to_df(in_list, out_list, pred_list):\n","  row = []\n","  for i in range(len(in_list)):\n","    batch_input = in_list[i]\n","    batch_output = out_list[i]\n","    batch_pred = pred_list[i]\n","    input = [j for j in batch_input if j != \"<pad>\" and j != \"<sos>\" and j != \"<eos>\" and j != \"<unk>\"]\n","    output = [j for j in batch_output if j != \"<pad>\" and j != \"<sos>\" and j != \"<eos>\" and j != \"<unk>\"]\n","    predict = [j for j in batch_pred if j != \"<pad>\" and j != \"<sos>\" and j != \"<eos>\" and j != \"<unk>\"]\n","    input_str = \"\".join(input)\n","    output_str =\"\".join(output)\n","    predict_str = \"\".join(predict)\n","    row.append([input_str, output_str, predict_str])\n","\n","  df = pd.DataFrame(row, columns=[\"input\",\"answer\",\"predict\"])\n","  df = df.sort_values('input')\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bM3sSB7ZITU9"},"source":["train_df = convert_list_to_df(train_input, train_output, train_pred)\n","val_df = convert_list_to_df(val_input, val_output, val_pred)\n","test_df = convert_list_to_df(test_input, test_output, test_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjRVwDVjjIUp"},"source":["df_s = pd.concat([train_df, test_df]).sort_values('input')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oh5GU1c3qOTO"},"source":["df_s.to_csv(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dGjeZakjPpf"},"source":[""],"execution_count":null,"outputs":[]}]}