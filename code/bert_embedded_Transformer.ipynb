{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_embedded_Transformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN3cpirpp+86nZ0pMWt6B41"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DNjZHQ_iEuFL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606279298068,"user_tz":-540,"elapsed":671,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"fe6a4508-c1a4-4fda-c45b-147748296ea0"},"source":["from google.colab import drive\n","drive.mount('/content/dirve')\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"execution_count":113,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/dirve; to attempt to forcibly remount, call drive.mount(\"/content/dirve\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M4ausvJbE4g_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606279300961,"user_tz":-540,"elapsed":3553,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"04c8e7a3-b205-4680-f11b-291ba59b00cf"},"source":["!pip install transformers fugashi mecab-python3 ipadic"],"execution_count":114,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n","Requirement already satisfied: fugashi in /usr/local/lib/python3.6/dist-packages (1.0.5)\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (1.0.3)\n","Requirement already satisfied: ipadic in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y4_ddJeRE83M","executionInfo":{"status":"ok","timestamp":1606279300962,"user_tz":-540,"elapsed":3548,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n","\n","class TransformerModel(nn.Module):\n","\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super(TransformerModel, self).__init__()\n","        self.model_type = 'Transformer'\n","        self.linear = nn.Linear(ninp ,32000)\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = bert_model.get_input_embeddings()\n","        self.ninp = ninp\n","        # self.decoder = bert_model.get_input_embeddings()\n","        self.decoder = nn.Embedding(ntoken, ninp)\n","        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers, norm=self.linear)\n","        #self.init_weights()\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        #self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, trg):\n","        trg_mask = model.generate_square_subsequent_mask(trg.size()[0]).to(device)\n","        # 分散表現に変換\n","        src = self.encoder(src)\n","        trg = self.decoder(trg)\n","        # 位置情報を入れる\n","        src = self.pos_encoder(src)\n","        trg = self.pos_encoder(trg)\n","        # モデルにデータを入れる\n","        output = self.transformer_encoder(src)\n","        # デコーダにエンコーダの出力を入れる（ここがおかしい）\n","        output = self.transformer_decoder(trg, output,tgt_mask = trg_mask)\n","        return output\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # print(x.size())\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"],"execution_count":115,"outputs":[]},{"cell_type":"code","metadata":{"id":"0yHD5NckFKvs","executionInfo":{"status":"ok","timestamp":1606279300963,"user_tz":-540,"elapsed":3544,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["from torchtext import data\n","from torchtext import datasets\n","from transformers import BertJapaneseTokenizer, BertForPreTraining\n","import random \n","import numpy as np"],"execution_count":116,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wo8yNDSdvNRb","executionInfo":{"status":"ok","timestamp":1606279300963,"user_tz":-540,"elapsed":3540,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":117,"outputs":[]},{"cell_type":"code","metadata":{"id":"jm_UKaNiFPE0","executionInfo":{"status":"ok","timestamp":1606279300964,"user_tz":-540,"elapsed":3537,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["tok = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')"],"execution_count":118,"outputs":[]},{"cell_type":"code","metadata":{"id":"oS_bkY2LFP_G","executionInfo":{"status":"ok","timestamp":1606279300966,"user_tz":-540,"elapsed":3535,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["def tokenizer(text):\n","  return tok.tokenize(text)"],"execution_count":119,"outputs":[]},{"cell_type":"code","metadata":{"id":"J83kvjO1FdWj","executionInfo":{"status":"ok","timestamp":1606279301201,"user_tz":-540,"elapsed":3766,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["SRC = data.Field(sequential=True, tokenize = tokenizer, init_token='<sos>', eos_token='<eos>', lower = True)"],"execution_count":120,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdGrZST1BXNj","executionInfo":{"status":"ok","timestamp":1606279301201,"user_tz":-540,"elapsed":3761,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["# 重複のないデータセットか重複のあるデータセットを選ぶ\n","# flagがTrueの時重複のないデータを返す\n","def choose_dataset(flag = False):\n","  if flag:\n","    return data.TabularDataset.splits(\n","        path=\"/content/dirve/My Drive/Colab Notebooks/data/\", train='one_train.tsv',\n","        validation='one_val.tsv', test='one_test.tsv', format='tsv',\n","        fields=[('SRC', SRC), ('TRG', SRC)]), \"/content/dirve/My Drive/Colab Notebooks/csv/one_result_bert_embedded_transformer.csv\"\n","  else:\n","    return data.TabularDataset.splits(\n","        path=\"/content/dirve/My Drive/Colab Notebooks/data/\", train='train.tsv',\n","        validation='val.tsv', test='test.tsv', format='tsv',\n","        fields=[('SRC', SRC), ('TRG', SRC)]), \"/content/dirve/My Drive/Colab Notebooks/csv/result_bert_embedded_transformer.csv\""],"execution_count":121,"outputs":[]},{"cell_type":"code","metadata":{"id":"8kDi0M2_Fier","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606279308617,"user_tz":-540,"elapsed":11171,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"b48eb3ea-9fdd-4813-a2e8-acde46d06e8e"},"source":["train, val, test, filename = choose_dataset(False)\n","SRC.build_vocab(train)\n","bert_model = BertForPreTraining.from_pretrained(\n","    \"cl-tohoku/bert-base-japanese\", # 日本語Pre trainedモデルの指定\n","    num_labels = 2, # ラベル数（今回はBinayなので2、数値を増やせばマルチラベルも対応可）\n","    output_attentions = False, # アテンションベクトルを出力するか\n","    output_hidden_states = True, # 隠れ層を出力するか\n",")"],"execution_count":122,"outputs":[{"output_type":"stream","text":["Some weights of BertForPreTraining were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese and are newly initialized: ['cls.predictions.decoder.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"N0yTQhdEGEB7","executionInfo":{"status":"ok","timestamp":1606279308617,"user_tz":-540,"elapsed":11165,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","train_batch_size = 100\n","test_batch_size = 100\n","eval_batch_size = 100\n","train_iter, val_iter, test_iter = data.BucketIterator.splits((train, val, test), sort = False,  batch_sizes = (train_batch_size,eval_batch_size, test_batch_size), device= device)"],"execution_count":123,"outputs":[]},{"cell_type":"code","metadata":{"id":"60fKoQ8NGGH6","executionInfo":{"status":"ok","timestamp":1606279308884,"user_tz":-540,"elapsed":11427,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["ntokens = 3996 # the size of vocabulary\n","emsize = 768 # embedding dimension\n","nhid = 768 # the dimension of the feedforward network model in nn.TransformerEncoder\n","nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n","nhead = 2 # the number of heads in the multiheadattention models\n","dropout = 0.3 # the dropout value\n","model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"],"execution_count":124,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLzdBFfpuQAP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606279308885,"user_tz":-540,"elapsed":11423,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"2e0fa16c-8ca2-40b7-a1a8-b4adf1239bbe"},"source":["model"],"execution_count":125,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TransformerModel(\n","  (linear): Linear(in_features=768, out_features=32000, bias=True)\n","  (pos_encoder): PositionalEncoding(\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n","        )\n","        (linear1): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=768, out_features=768, bias=True)\n","        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n","        )\n","        (linear1): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=768, out_features=768, bias=True)\n","        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","      )\n","    )\n","  )\n","  (encoder): Embedding(32000, 768, padding_idx=0)\n","  (decoder): Embedding(3996, 768)\n","  (transformer_decoder): TransformerDecoder(\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n","        )\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n","        )\n","        (linear1): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=768, out_features=768, bias=True)\n","        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","        (dropout3): Dropout(p=0.3, inplace=False)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n","        )\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n","        )\n","        (linear1): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=768, out_features=768, bias=True)\n","        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","        (dropout3): Dropout(p=0.3, inplace=False)\n","      )\n","    )\n","    (norm): Linear(in_features=768, out_features=32000, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"code","metadata":{"id":"F_mXeTeaGG7V","executionInfo":{"status":"ok","timestamp":1606279308886,"user_tz":-540,"elapsed":11417,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["criterion = nn.CrossEntropyLoss(ignore_index=SRC.vocab.stoi[\"<pad>\"])\n","lr = 5 # learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n","\n","import time\n","def train(iterator):\n","    model.train() # Turn on the train mode\n","    total_loss = 0.\n","    start_time = time.time()\n","    for i, batch in enumerate(iterator):\n","        src = batch.SRC\n","        trg = batch.TRG\n","        optimizer.zero_grad()\n","        output = model(src, trg)\n","        #print(\"output from model size:\", output.size())\n","        #print(\"targets of unsqueezed size:\", trg.size())\n","        output = output[:].view(-1, output.shape[-1])\n","        trg = trg[:].view(-1)\n","        #print(\"output size:\", output.size())\n","        #print(\"targets size:\", trg.size())\n","        loss = criterion(output, trg)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        \n","\n","def evaluate(eval_model, data_source):\n","    eval_model.eval() # Turn on the evaluation mode\n","    total_loss = 0.\n","    with torch.no_grad():\n","      for i, batch in enumerate(data_source):\n","        data = batch.SRC\n","        targets = batch.TRG\n","        #src_mask = model.generate_square_subsequent_mask(data.shape[0]).to(device)\n","        output = eval_model(data, targets)\n","        output_flat = output[:].view(-1, output.shape[-1])\n","        targets = targets[:].view(-1)\n","        total_loss += len(data) * criterion(output_flat, targets).item()\n","    return total_loss / (len(data_source) - 1)"],"execution_count":126,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoPF710mGM1u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606280269630,"user_tz":-540,"elapsed":972156,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"2ceffaa7-f395-4578-d9ce-7b26ebbf1cda"},"source":["best_val_loss = float(\"inf\")\n","epochs = 20 # The number of epochs\n","best_model = None\n","model.init_weights()\n","\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train(train_iter)\n","    val_loss = evaluate(model, val_iter)\n","    print('-' * 89)\n","    print('| epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","          .format(epoch, (time.time() - epoch_start_time), val_loss))\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_model = model\n","\n","    scheduler.step()"],"execution_count":127,"outputs":[{"output_type":"stream","text":["-----------------------------------------------------------------------------------------\n","| epoch   1 | time: 49.79s | valid loss 339.36 | \n","-----------------------------------------------------------------------------------------\n","| epoch   2 | time: 47.83s | valid loss 368.12 | \n","-----------------------------------------------------------------------------------------\n","| epoch   3 | time: 48.39s | valid loss 343.00 | \n","-----------------------------------------------------------------------------------------\n","| epoch   4 | time: 47.97s | valid loss 223.44 | \n","-----------------------------------------------------------------------------------------\n","| epoch   5 | time: 48.29s | valid loss 219.59 | \n","-----------------------------------------------------------------------------------------\n","| epoch   6 | time: 48.29s | valid loss 239.57 | \n","-----------------------------------------------------------------------------------------\n","| epoch   7 | time: 47.94s | valid loss 149.42 | \n","-----------------------------------------------------------------------------------------\n","| epoch   8 | time: 47.74s | valid loss 132.27 | \n","-----------------------------------------------------------------------------------------\n","| epoch   9 | time: 47.76s | valid loss 121.33 | \n","-----------------------------------------------------------------------------------------\n","| epoch  10 | time: 48.05s | valid loss 111.00 | \n","-----------------------------------------------------------------------------------------\n","| epoch  11 | time: 47.61s | valid loss 108.33 | \n","-----------------------------------------------------------------------------------------\n","| epoch  12 | time: 48.21s | valid loss 93.87 | \n","-----------------------------------------------------------------------------------------\n","| epoch  13 | time: 47.84s | valid loss 84.61 | \n","-----------------------------------------------------------------------------------------\n","| epoch  14 | time: 47.61s | valid loss 94.08 | \n","-----------------------------------------------------------------------------------------\n","| epoch  15 | time: 47.90s | valid loss 84.48 | \n","-----------------------------------------------------------------------------------------\n","| epoch  16 | time: 47.92s | valid loss 91.19 | \n","-----------------------------------------------------------------------------------------\n","| epoch  17 | time: 47.75s | valid loss 80.13 | \n","-----------------------------------------------------------------------------------------\n","| epoch  18 | time: 47.71s | valid loss 83.38 | \n","-----------------------------------------------------------------------------------------\n","| epoch  19 | time: 48.08s | valid loss 68.02 | \n","-----------------------------------------------------------------------------------------\n","| epoch  20 | time: 47.83s | valid loss 62.20 | \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dc_W5qalGRQa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606280270545,"user_tz":-540,"elapsed":973064,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"3a7af8a7-0de5-493a-9b93-6a780e7bd2d6"},"source":["test_loss = evaluate(best_model, test_iter)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} |'.format(\n","    test_loss))\n","print('=' * 89)"],"execution_count":128,"outputs":[{"output_type":"stream","text":["=========================================================================================\n","| End of training | test loss 61.75 |\n","=========================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PWTpNLRGGVDC","executionInfo":{"status":"ok","timestamp":1606280272003,"user_tz":-540,"elapsed":974518,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["torch.save(best_model.state_dict(), \"/content/dirve/My Drive/Colab Notebooks/model/bert_embedded_transformer.pth\")"],"execution_count":129,"outputs":[]},{"cell_type":"code","metadata":{"id":"6FbrKm90aurT"},"source":["model.state_dict(torch.load(\"/content/dirve/My Drive/Colab Notebooks/model/bert_embedded_transformer.pth\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0bTLVRfCIaUP","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1606280273550,"user_tz":-540,"elapsed":976057,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"4e627263-61b8-43c4-b7c8-4a1c531612f6"},"source":["\"\"\"\n","def gen_sentence(sentence, src_field, trg_field, model, max_len = 50):\n","  model.eval()\n","\n","  tokens = [src_field.init_token] + tokenizer(sentence) + [src_field.eos_token]\n","  \n","  src_index = [src_field.vocab.stoi[i] for i in tokens]\n","  src_tensor = torch.LongTensor(src_index).unsqueeze(0).to(device)\n","  # src_len = torch.LongTensor([len(src_index)]).to(device)\n","  src_tensor = model.encoder(src_tensor)\n","  src_tensor = mode.pos_encoder(src_tensor)\n","  with torch.no_grad():\n","    enc_output = model.transformer_encoder(src_tensor)\n","  \n","  trg_index = [trg_field.vocab.stoi[trg_field.init_token]]\n","  for i in range(max_len):\n","    trg_tensor = torch.LongTensor(trg_index[-1]).unsqueeze(2).to(device)\n","    trg_tensor = model.encoder(trg_tensor)\n","    trg_tensor = model.pos_encoder(trg_tensor)\n","    with torch.no_grad():\n","      output = model.transformer_deocder(trg_tensor, enc_output)\n","    \n","    pred_token = output.argmax(1).item()\n","    trg_index.append(pred_token)\n","    if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","      break\n","\n","  trg_tokens = [trg_field.vocab.itos[i] for i in trg_index]\n","  return trg_tokens\n","\n","  def gen_sentence_list(path): \n","  col, pred = [], []\n","  input, output = [], []\n","  with open(path, mode = 'r') as f:\n","    for file_list in f:\n","      col.append(file_list.split('\\t'))\n","  for i in col:\n","    input.append(i[0])\n","    output.append(i[1])\n","\n","  for sentence in input:\n","    pred.append(gen_sentence(sentence, SRC, SRC, model))\n","  return input, output, pred\n","\n","path = \"/content/dirve/My Drive/Colab Notebooks/data/test.tsv\"\n","test_input, test_output, test_pred = gen_sentence_list(path)\n","path = \"/content/dirve/My Drive/Colab Notebooks/data/train.tsv\"\n","train_input, train_output, train_pred = gen_sentence_list(path)\n","path = \"/content/dirve/My Drive/Colab Notebooks/data/val.tsv\"\n","val_input, val_output, val_pred = gen_sentence_list(path)\n","  \"\"\""],"execution_count":131,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef gen_sentence(sentence, src_field, trg_field, model, max_len = 50):\\n  model.eval()\\n\\n  tokens = [src_field.init_token] + tokenizer(sentence) + [src_field.eos_token]\\n  \\n  src_index = [src_field.vocab.stoi[i] for i in tokens]\\n  src_tensor = torch.LongTensor(src_index).unsqueeze(0).to(device)\\n  # src_len = torch.LongTensor([len(src_index)]).to(device)\\n  src_tensor = model.encoder(src_tensor)\\n  src_tensor = mode.pos=encoder(src_tensor)\\n  with torch.no_grad():\\n    enc_output = model.transformer_encoder(src_tensor)\\n  \\n  trg_index = [trg_field.vocab.stoi[trg_field.init_token]]\\n  for i in range(max_len):\\n    trg_tensor = torch.LongTensor([trg_index[-1]]).to(device)\\n    with torch.no_grad():\\n      output = model.transformer_deocder(trg_tensor, enc_output)\\n    \\n    pred_token = output.argmax(1).item()\\n    trg_index.append(pred_token)\\n    if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\\n      break\\n\\n  trg_tokens = [trg_field.vocab.itos[i] for i in trg_index]\\n  return trg_tokens\\n\\n  def gen_sentence_list(path): \\n  col, pred = [], []\\n  input, output = [], []\\n  with open(path, mode = \\'r\\') as f:\\n    for file_list in f:\\n      col.append(file_list.split(\\'\\t\\'))\\n  for i in col:\\n    input.append(i[0])\\n    output.append(i[0])\\n\\n  for sentence in input:\\n    pred.append(gen_sentence(sentence, SRC, SRC, model))\\n  return input, output, pred\\n\\npath = \"/content/dirve/My Drive/Colab Notebooks/data/test.tsv\"\\ntest_input, test_output, test_pred = gen_sentence_list(path)\\npath = \"/content/dirve/My Drive/Colab Notebooks/data/train.tsv\"\\ntrain_input, train_output, train_pred = gen_sentence_list(path)\\npath = \"/content/dirve/My Drive/Colab Notebooks/data/val.tsv\"\\nval_input, val_output, val_pred = gen_sentence_list(path)\\n  '"]},"metadata":{"tags":[]},"execution_count":131}]},{"cell_type":"code","metadata":{"id":"mvpBPYSczFm0","executionInfo":{"status":"ok","timestamp":1606280273551,"user_tz":-540,"elapsed":976053,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["def gen_sentence(sentence, src_field, trg_field, model, batch_size):\n","  model.eval()\n","  in_str, out_str, pred, tmp = [], [], [], []\n","  length = len(sentence)\n","\n","  with torch.no_grad():\n","    for _, batch in enumerate(sentence):\n","      src = batch.SRC\n","      trg = batch.TRG\n","      output = model(src, trg)\n","          \n","      for j in range(min(length, batch_size)):\n","        _, topi = output.data.topk(1)\n","        _, topi_s = output.data.topk(2) \n","        for k in range(topi.size()[1]):\n","          if topi[:, k][0] == trg_field.vocab.stoi[\"<eos>\"]:\n","            for m in range(topi_s.size()[0]):\n","              for l in range(topi_s.size()[1]):\n","                topi[m][l][0] = topi_s[m][l][1]\n","          for i in range(topi.size()[0]):\n","            if trg_field.vocab.itos[topi[:, k][i]] == \"<eos>\":\n","              break\n","            tmp.append(trg_field.vocab.itos[topi[:, k][i]])\n","          pred.append(tmp)\n","          tmp = []\n","        #print(src.size())\n","        in_str.append([src_field.vocab.itos[i.item()] for i in src[:,j] if src_field.vocab.itos[i.item()] != \"<eos>\"])\n","        out_str.append([trg_field.vocab.itos[i.item()] for i in trg[:,j] if trg_field.vocab.itos[i.item()] != \"<eos>\"])\n","      \n","  return in_str, out_str, pred"],"execution_count":132,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOwWmtaKIbdG","executionInfo":{"status":"ok","timestamp":1606281238917,"user_tz":-540,"elapsed":1941415,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["# 中間発表時にはテストデータは用いない\n","test_in, test_out, test_pred = [],[],[]\n","test_in, test_out, test_pred = gen_sentence(test_iter, SRC, SRC, best_model, test_batch_size)\n","val_in, val_out, val_pred = [],[],[]\n","val_in, val_out, val_pred = gen_sentence(val_iter, SRC, SRC, model, eval_batch_size)\n","train_in, train_out, train_pred = [],[],[]\n","train_in, train_out, train_pred = gen_sentence(train_iter, SRC, SRC, model, train_batch_size)"],"execution_count":133,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-MkG5DWMEgO","executionInfo":{"status":"ok","timestamp":1606281238919,"user_tz":-540,"elapsed":1941412,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["import pandas as pd"],"execution_count":134,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-x0Nb8VMOnd","executionInfo":{"status":"ok","timestamp":1606281238919,"user_tz":-540,"elapsed":1941408,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["def convert_list_to_df(in_list, out_list, pred_list):\n","  row = []\n","  for i in range(len(in_list)):\n","    batch_input = in_list[i]\n","    batch_output = out_list[i]\n","    batch_pred = pred_list[i]\n","    input = [j for j in batch_input if j != \"<pad>\" and j != \"<sos>\" and j != \"<eos>\" and j != \"<unk>\"]\n","    output = [j for j in batch_output if j != \"<pad>\" and j != \"<sos>\" and j != \"<eos>\" and j != \"<unk>\"]\n","    predict = [j for j in batch_pred if j != \"<pad>\" and j != \"<sos>\" and j != \"<eos>\" and j != \"<unk>\"]\n","    input_str = \"\".join(input).replace(\"#\", \"\")\n","    output_str =\"\".join(output).replace(\"#\", \"\")\n","    predict_str = \"\".join(predict).replace(\"#\", \"\")\n","    row.append([input_str, output_str, predict_str])\n","\n","  df = pd.DataFrame(row, columns=[\"input\",\"answer\",\"predict\"])\n","  df = df.sort_values('input')\n","  return df"],"execution_count":135,"outputs":[]},{"cell_type":"code","metadata":{"id":"RzNMZFQBMYRx","executionInfo":{"status":"ok","timestamp":1606281239167,"user_tz":-540,"elapsed":1941653,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["train_df = convert_list_to_df(train_in, train_out, train_pred)\n","val_df = convert_list_to_df(val_in, val_out, val_pred)\n","test_df = convert_list_to_df(test_in, test_out, test_pred)"],"execution_count":136,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpqTaFLqMZ95","executionInfo":{"status":"ok","timestamp":1606281239167,"user_tz":-540,"elapsed":1941649,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["df_s = pd.concat([train_df, test_df]).sort_values('input')"],"execution_count":137,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-25c9orMc7q","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1606281239169,"user_tz":-540,"elapsed":1941647,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}},"outputId":"5586e871-3111-4686-8c81-60a697a90981"},"source":["df_s.head(10)"],"execution_count":138,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>answer</th>\n","      <th>predict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>18359</th>\n","      <td>11時半頃</td>\n","      <td>十一時半に</td>\n","      <td>あん</td>\n","    </tr>\n","    <tr>\n","      <th>14143</th>\n","      <td>11時半頃</td>\n","      <td>いらっしゃったうーん</td>\n","      <td>うーん</td>\n","    </tr>\n","    <tr>\n","      <th>3813</th>\n","      <td>15分ぐらいまで</td>\n","      <td>十五分</td>\n","      <td>あであー</td>\n","    </tr>\n","    <tr>\n","      <th>19914</th>\n","      <td>15分ぐらいまで</td>\n","      <td>あっ</td>\n","      <td>あー</td>\n","    </tr>\n","    <tr>\n","      <th>19074</th>\n","      <td>15分ぐらいまで駅に</td>\n","      <td>15分はい</td>\n","      <td>あーそうなんですね</td>\n","    </tr>\n","    <tr>\n","      <th>1702</th>\n","      <td>15分ぐらいまで駅に</td>\n","      <td>あはい</td>\n","      <td>あはははそうなんですね</td>\n","    </tr>\n","    <tr>\n","      <th>6629</th>\n","      <td>15分ぐらいまで駅に着くまでにかかりました</td>\n","      <td>はいだったんですねありがとう</td>\n","      <td>あーうーんうん</td>\n","    </tr>\n","    <tr>\n","      <th>4196</th>\n","      <td>15分ぐらいまで駅に着くまでにかかりました</td>\n","      <td>ええあそうですねそうだったんですね</td>\n","      <td>はーそうなんですか</td>\n","    </tr>\n","    <tr>\n","      <th>8474</th>\n","      <td>15分ぐらいまで駅に着くまでにかかりました</td>\n","      <td>あそうですか</td>\n","      <td>はーしのしのなんですね</td>\n","    </tr>\n","    <tr>\n","      <th>15389</th>\n","      <td>15分ぐらいまで駅に着くまでにかかりました</td>\n","      <td>はいうーんうん</td>\n","      <td>でですねしのねーうーん</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       input             answer      predict\n","18359                  11時半頃              十一時半に           あん\n","14143                  11時半頃         いらっしゃったうーん          うーん\n","3813                15分ぐらいまで                十五分         あであー\n","19914               15分ぐらいまで                 あっ           あー\n","19074             15分ぐらいまで駅に              15分はい    あーそうなんですね\n","1702              15分ぐらいまで駅に                あはい  あはははそうなんですね\n","6629   15分ぐらいまで駅に着くまでにかかりました     はいだったんですねありがとう      あーうーんうん\n","4196   15分ぐらいまで駅に着くまでにかかりました  ええあそうですねそうだったんですね    はーそうなんですか\n","8474   15分ぐらいまで駅に着くまでにかかりました             あそうですか  はーしのしのなんですね\n","15389  15分ぐらいまで駅に着くまでにかかりました            はいうーんうん  でですねしのねーうーん"]},"metadata":{"tags":[]},"execution_count":138}]},{"cell_type":"code","metadata":{"id":"omMdo3neMa2x","executionInfo":{"status":"ok","timestamp":1606281239769,"user_tz":-540,"elapsed":1942244,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":["df_s.to_csv(filename)"],"execution_count":139,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxEw_8ClQDI8","executionInfo":{"status":"ok","timestamp":1606281239770,"user_tz":-540,"elapsed":1942241,"user":{"displayName":"守山慧","photoUrl":"","userId":"18276650506428796801"}}},"source":[""],"execution_count":139,"outputs":[]}]}