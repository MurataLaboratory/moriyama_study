{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from janome.tokenizer import Tokenizer\n",
    "j_t = Tokenizer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok for tok in j_t.tokenize(text, wakati = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(path):\n",
    "    data = pd.read_csv(path)\n",
    "    ans = data[\"answer\"]\n",
    "    pred = data[\"predict\"]\n",
    "    cnt = 0\n",
    "    # 一致率\n",
    "    for i in range(len(ans)):\n",
    "        if ans[i] == pred[i]:\n",
    "            cnt+=1\n",
    "    same_percentage = (cnt/len(ans))*100\n",
    "    # 種類数\n",
    "    col = []\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] not in col:\n",
    "            col.append(pred[i])\n",
    "    ans_kind = len(col)\n",
    "    # bleu\n",
    "    df_ans = data.assign(bleu = 0)\n",
    "    df_ans = df_ans.groupby(\"input\").agg({\n",
    "    'answer':list,\n",
    "    \"predict\": list,\n",
    "    \"bleu\" : list\n",
    "    })\n",
    "    df_ans = df_ans.reset_index()\n",
    "    col, ans, pred = [], [], []\n",
    "    for k in range(len(df_ans)):\n",
    "        for j in range(len(df_ans[\"predict\"][k])):\n",
    "\n",
    "            for i in range(len(df_ans[\"answer\"][k])):\n",
    "                ans_str = df_ans[\"answer\"][k][i]\n",
    "                ans.append(tokenizer(ans_str))\n",
    "\n",
    "            #print(k)\n",
    "            pred = df_ans[\"predict\"][k][j]\n",
    "            #print(pred)\n",
    "            pred = tokenizer(pred)\n",
    "            col.append(nltk.translate.bleu_score.sentence_bleu(ans, pred))\n",
    "            ave = sum(col)/len(col)\n",
    "            #print(ave)\n",
    "            df_ans[\"bleu\"][k][j] = ave\n",
    "            col, ans = [], []\n",
    "    input_str, answer, predict, bleu = [], [], [], []\n",
    "    for i in range(len(df_ans)):\n",
    "        for j in range(len(df_ans[\"answer\"][i])):\n",
    "            input_str.append(df_ans[\"input\"][i])\n",
    "            answer.append(df_ans[\"answer\"][i][j])\n",
    "            predict.append(df_ans[\"predict\"][i][j])\n",
    "            bleu.append(df_ans[\"bleu\"][i][j])\n",
    "            \n",
    "    df = pd.DataFrame({\"input\":input_str,\"answer\":answer,\"predict\":predict,\"bleu\":bleu})\n",
    "    path = path.replace(\"result\", \"score\")\n",
    "    df.to_csv(path)\n",
    "    blue = df[\"bleu\"]\n",
    "    l = blue.values.tolist()\n",
    "    ave_blue = sum(l)/len(l)\n",
    "    return same_percentage,ans_kind, ave_blue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keisc\\anaconda3\\envs\\study\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\keisc\\anaconda3\\envs\\study\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\keisc\\anaconda3\\envs\\study\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.2866603595080417, 75, 0.021538383243622228)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"csv/result_seq2seq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\"csv/result_transformer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
